#Calling Chat Models
from langchain.chat_models import ChatOpenAI
from langchain.schma import (
	AIMessage,
	HumanMessage,
	SystemMessage
)

chat = ChatOpenAI()

chat([HumanMessage(content = "Translate this sentence to French: I love programming.")])



#Async API
messages = [[
	SystemMessage(content = "You are a helpful assistant that translates English to French."),
	HumanMessage(content = "I love programming.")
]]

result = chat.agenerate(messages)



#Streaming Response
from langchain.chat_models import ChatOpenAI
from langchain.schma import HumanMessage

from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

chat = ChatOpenAI(
	streaming = True,
	callbacks = [StreamingStdOutCallbackHandler()],
	temperature = 0
)
resp = chat([HumanMessage(content = "Write me a song about sparkling water.")])



#Caching
from langchain.globals import set_llm_cache
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI()

from langchain.cache import InMemoryCache
set_llm_cache(InMemoryCache())

llm.predict("Tell me a joke")



#Prompt Templates: Parametrized Model Inputs
template = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {content}."
)
template.format(adjective = "funny", content = "chickens")
###
template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI bot. Your name is {name}."),
    ("human", "Hello, how are you doing?"),
    ("ai", "I'm doing well, thanks!"),
    ("human", "{user_input}"),
])

messages = template.format_messages(
    name = "Bob",
    user_input = "What is your name?"
)


#Few-shot prompt templates
example = [
    {"input": "2+2", "output": "4"},
    {"input": "2+3", "output": "5"},
]

example_prompt = ChatPromptTemplate.from_messages(
    [('human', '{input}'), ('ai', '{output}')]
)

few_shot_prompt = FewShotChatMessagePromptTemplate(
    exmaples = examples,
    example_prompt = example_prompt,
)

final_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', 'You are a helpful AI Assistant'),
        few_shot_prompt,
        ('human', '{input}'),
	]
)
final_prompt.format(input = "What is 4+4?")



#Template Formats
#jinja2
jinja2_template = "Tell me a {{ adjective }} joke about {{ content}}"
prompt = PromptTemplate.from_template(jinja2_template, template_format = "jinja2")

prompt.format(adjective = "funny", content = "chickens")

#f-string
fstring_template = "Tell me a {adjective} joke about {content}"
prompt = PromptTemplate.from_template(fstring_template)

prompt.format(adjective = "funny", content = "chickens")



#Partial Prompt
prompt = PromptTemplate(
    template = "Tell me a {adjective} joke about {content}",
    input_variables = ["adjective", "content"]
)

partial_prompt = prompt.partial(adjective = "funny");
...
print(partial_prompt.format(content = "dog"))



#Document Loaders
from langchain.document_loaders import TextLoader

loader = DirectoryLoader(
    '../',
    glob = "**/*.md",
    loader_cls = TextLoader
)

docs = loader.load()



#Document transformers: Text splitters
documents = TextLoader("./state_of_the_union.txt").load()

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size = 500,
    chunk_overlap = 10
)

texts = text_splitter.split_documents(documents)


#Text embedding models
from langchain.embeddings import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()

embeddings = embeddings_model.embed_documents([
    "Hi there!",
    "Oh, hello!",
    "What's your name?",
])

embedded_query = embeddings_model.embed_query("What is your name?")



#Vector stores
raw_documents = TextLoader('./state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)
documents = text_splitter.split_documents(raw_documents)
db = Chroma.from_documents(documents, OpenAIEmbeddings())

query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)



#Retrievers
db = FAISS.from_documents(texts, embeddings)

retriever = db.as_retriever()
docs = retriever.get_relevant_documents("what did he say about jackson")



#Chains
class Chain(BaseModel, ABC):
    """Base interface that all chains should implement."""
    
	memory: BaseMemory
    callbacks: Callbacks
    
	def __call__(
		self,
		inputs: Any,
		return_only_outputs: bool = False,
		callbacks: Callbacks = None,
	) 	-> Dict[str, Any]:



#LangChain Expression Language
vectorstore = FAISS.from_texts(["harrison worked at kensho"], embeddings = OpenAIEmbeddings())
retriever = vectorstore.as_retriever()

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()

chain = (
	{"context": retriever, "question": RunnablePassthrough()}
	| prompt
	| model
	| StrOutputParser()
)



#Add Memory to Chatbot
prompt = ChatPromptTemplate.from_messages([
	SystemMessage(content = "You are a chatbot having a conversation with a human."),
	MessagesPlaceholder(variable_name = "chat_history"), #The memory will be stored.
	HumanMessagePromptTemplate.from_template("{human_input}"), #The human input will be injected
])

memory = ConversationBufferMemory(memory_key = "chat_history", return_messages = True)

llm = ChatOpenAI()

chat_llm_chain = LLMChain(
	llm = llm,
	prompt = prompt,
	verbose = True,
	memory = memory,
)

chat_llm_chain.predict(human_input = "Hi there my friend")